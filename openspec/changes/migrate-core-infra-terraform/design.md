# Design: Multi-Cloud Terraform Infrastructure

## Context

The k0rdent Azure setup project uses bash scripts with Azure CLI to provision infrastructure. The system needs to:
1. Support both Azure and AWS deployments from a single codebase
2. Maintain cloud-init/user-data customization per VM/instance
3. Integrate with existing WireGuard/k0s/k0rdent orchestration scripts
4. Handle spot instances with proper retry/recovery logic
5. Track deployment state across infrastructure and runtime layers

Current bash approach works but lacks state management, drift detection, and consistent multi-cloud patterns.

## Goals

- Replace imperative Azure CLI scripts with declarative Terraform modules
- Establish framework for AWS support with identical workflow patterns
- Maintain bash orchestration for WireGuard, k0s, k0rdent (out of Terraform scope)
- Provide clear handoff between Terraform (infrastructure) and bash (configuration)
- Enable remote state management with locking for team collaboration

## Non-Goals

- Moving WireGuard/k0s/k0rdent installation into Terraform (stays in bash)
- Full conversion of all scripts in first phase (child clusters can follow later)
- Removing bash entirely (bash remains orchestrator, Terraform manages infrastructure)
- Creating complex Terraform abstractions (keep modules simple and focused)

## Decisions

### 1. Module Structure: Provider-Specific Directories

**Decision**: Organize modules by cloud provider (terraform/modules/azure/, terraform/modules/aws/)

**Alternatives Considered**:
- Single unified module with provider conditionals → Too complex, hard to test
- Separate Terraform roots per cloud → Code duplication, harder to maintain

**Rationale**: Provider-specific modules keep code simple while enabling shared patterns. Root module selects provider via variable.

### 2. Configuration Flow: YAML → Terraform → Outputs

**Decision**: Generate terraform.tfvars.json from k0rdent.yaml via bin/configure.sh

**Alternatives Considered**:
- Direct YAML parsing in Terraform → Requires external data source, adds complexity
- Manual tfvars creation → Error-prone, breaks existing workflow

**Rationale**: Existing configure.sh already parses YAML; extend it to emit tfvars. Keeps Terraform inputs standard and tooling-friendly.

### 3. Cloud-Init Handling: Generated by Bash, Consumed by Terraform

**Decision**: prepare-deployment.sh generates cloud-init files → Terraform reads as input

**Alternatives Considered**:
- Template cloud-init in Terraform → Loses bash flexibility for WireGuard key injection
- Fully dynamic cloud-init in Terraform templatefile() → Complex variable passing

**Rationale**: Cloud-init generation already handles WireGuard keys, SSH config, VM-specific settings. Terraform consumes as file input (minimal change).

### 4. State Integration: Terraform Outputs → deployment-state.yaml

**Decision**: terraform-wrapper.sh extracts outputs (JSON) and updates deployment-state.yaml

**Alternatives Considered**:
- Read Terraform state directly in bash → Tightly couples to state format
- Separate Terraform state and YAML state → Dual source of truth issues

**Rationale**: Outputs are stable API. Bash scripts continue using state-management.sh functions; those functions prefer Terraform outputs when available.

### 5. VM Creation: Terraform Handles Spot Retry via Lifecycle

**Decision**: Use Terraform lifecycle create_before_destroy for spot instances

**Alternatives Considered**:
- Keep bash retry logic → Defeats purpose of using Terraform
- No retry, rely on manual reapply → Poor UX for spot evictions

**Rationale**: Terraform's lifecycle management + proper timeouts handle transient failures. Spot evictions during apply trigger automatic retry.

### 6. Multi-Cloud Selection: Single Root with Provider Variable

**Decision**: Use var.cloud_provider (azure|aws) to select module and provider

```hcl
variable "cloud_provider" {
  type    = string
  default = "azure"
}

module "azure_infra" {
  count  = var.cloud_provider == "azure" ? 1 : 0
  source = "./modules/azure"
  # ...
}

module "aws_infra" {
  count  = var.cloud_provider == "aws" ? 1 : 0
  source = "./modules/aws"
  # ...
}
```

**Alternatives Considered**:
- Workspaces → Confusing state management across clouds
- Separate directories → More duplication

**Rationale**: Conditional module instantiation keeps single workflow with explicit provider choice.

### 7. Remote State: Provider-Specific Backends

**Decision**: Support Azure Storage (azurerm backend) and S3 (s3 backend) via backend config

```bash
# Azure
terraform init -backend-config="storage_account_name=..." \
               -backend-config="container_name=tfstate" \
               -backend-config="key=${K0RDENT_CLUSTERID}.tfstate"

# AWS
terraform init -backend-config="bucket=..." \
               -backend-config="key=${K0RDENT_CLUSTERID}.tfstate" \
               -backend-config="dynamodb_table=terraform-locks"
```

**Alternatives Considered**:
- Local state only → No locking, not team-friendly
- Terraform Cloud → Requires SaaS account, not self-hosted

**Rationale**: Native backends work with existing cloud credentials. terraform-wrapper.sh auto-configures based on cloud_provider.

## Architecture

```
┌─────────────────────┐
│  k0rdent.yaml       │  User edits configuration
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│ bin/configure.sh    │  Generate terraform.tfvars.json
│   export            │  + deployment-state.yaml init
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│ bin/terraform-      │  terraform init/plan/apply
│   wrapper.sh        │  Extract outputs → state
└──────┬──────────────┘
       │
       ├─► Terraform ──► Azure/AWS APIs
       │     Modules       (create infra)
       │
       ▼
┌─────────────────────┐
│ deployment-state    │  Stores Terraform outputs
│   .yaml             │  (VM IPs, resource names)
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│ prepare-deployment  │  Generate WireGuard keys
│ manage-vpn          │  + cloud-init files
│ install-k0s         │  Read IPs from state
│ install-k0rdent     │  Deploy software stack
└─────────────────────┘
```

## Migration Plan

### Phase 1: Azure Module Development (2 weeks)
- Create modules/azure/{network,compute}
- Implement terraform-wrapper.sh
- Extend configure.sh for tfvars generation
- Test with single-node deployment

### Phase 2: Integration Testing (1 week)
- End-to-end deployment with Terraform + bash orchestration
- Spot instance failure scenarios
- Multi-zone controller + worker deployments
- State migration from existing bash deployment

### Phase 3: AWS Framework (1 week)
- Create modules/aws/{network,compute} mirroring Azure patterns
- Test AWS EC2 deployment (may use existing AWS child cluster config)
- Validate provider selection logic

### Phase 4: Documentation & Transition (1 week)
- Write migration guide for existing deployments
- Document remote state setup procedures
- Create terraform import scripts for legacy resources
- Add --legacy flag for gradual adoption

## Risks & Trade-offs

### Risk: Terraform Learning Curve
**Mitigation**: Provide Makefile with common commands, terraform-wrapper.sh abstracts complexity

### Risk: State File Corruption
**Mitigation**: Use remote state with locking from day one; document backup/recovery procedures

### Risk: Cloud-Init Size Limits
**Mitigation**: Terraform supports file() and filebase64() for large user-data; test with current cloud-init sizes

### Risk: Spot Instance Handling Differences
**Mitigation**: Test spot evictions during apply/refresh; document expected behavior vs bash approach

### Risk: Breaking Existing Deployments
**Mitigation**: --legacy flag maintains bash path; terraform import available for resource adoption

## Open Questions

1. **Remote state sharing**: Team vs individual deployments? → Individual with optional team state
2. **Terraform version**: Pin to 1.5+ for new features? → Yes, use required_version constraint
3. **Module versioning**: How to version modules? → Git tags initially, Terraform registry later
4. **Child clusters**: Migrate to Terraform or keep bash? → Keep bash initially, migrate in Phase 5
